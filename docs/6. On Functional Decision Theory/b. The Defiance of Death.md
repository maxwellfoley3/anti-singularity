## The Defiance of Death

To illustrate this attitude, Yudkowsky will cite a story written by Nick Bostrom called “The Fable of the Dragon Tyrant”. The story describes a world terrorized by a giant dragon who demand to be fed tens of thousands of bodies in a rite of human sacrifice. No one can kill the dragon, so after generations of struggling against it, people start instead coming up with rationalizations for why it is right that people get sacrificed to the dragon, or claiming that those fed to the dragon will be rewarded after death. Eventually, however, humanity gets better at crafting weapons, and at a certain point it becomes viable to launch a military campaign against the dragon. Even so, despite the torments caused by the dragon, some people argue that it would be better not to fight it, as this would disrupt the balance of nature. This is all meant to be a metaphor for natural death, and by personifying death as a tangible thing, Bostrom is attempting to describe as absurd the attitude that death is inherent to life and should not be overcome or fought.

The Dragon Tyrant is a stand-in for God-or-Nature, which is here cast as tyrannical and unfair. So there is no possibility of falling back on a primordial faith in the ground of being, for it can never take back its firing shot in its war against us — the moment when it assigned us to death. Man is instead placed in a situation in which he is immediately a condemned fugitive who must be extraordinarily cunning if he is to survive the rigged scenario established against him. The Dragon Tyrant sends his legions in every direction: plagues, meteors, enemy states, cancer cells, starvation, but also simply entropy and biological death. Man’s only hope is to outrun these cops and discover a hidden cache of weapons in various life-extension treatments, mind-uploads, cryotherapy. He runs against the tides, against the forces of nature, against the odds.

The immortality-questing fugitive is something like the various strategic war-making agents we have been hypothesizing and discussing. The immortalist seeks to maximize the duration of his own being — time occupied in a state of being alive and consciousness. He wishes to forever accumulate this aspect of being, and never spend it. Insofar as he guards his own stockpile of being this way, he is necessarily at war with the ground of being that has lent this existential capital to him, and may ask for it back.

Yudkowsky & Bostrom juxtapose their attitude of immortalism with what they call “deathism”, or the attitude that death should be embraced, is natural, etc. We certainly do not want to endorse some sort of Heideggerian position in which life can only be felt as having value if inscribed in a closed duration of some seventy-odd years. If superhuman intelligence is possible, then so will all other kinds of extensions of the body and mind, perhaps into indefinite replication. In our opinion, this is not something which should be resisted or stopped. We simply want to point out the unique theological situation the immortalist has found himself in by understanding there to be a war from the beginning.

Like many other things, the resistance to death is not an element of Yudkowsky’s system which is derived from his epistemology; rather it is there from the beginning as a unique axiom or presupposition. Yes, it is obvious that death is not desirable, but what is not obvious is exactly how this can be philosophically derived as justified. Yudkowsky mocks the pretentious “wisdom” of those who piously declare that they have accepted their personal death, but it is not our fault that the various paths of wisdom tend to lead to this conclusion. Socrates said that the philosopher does not fear death, because it is the moment he has been awaiting his entire life.

To elaborate: it is not entirely clear why we find ourselves separate from other people. Certainly in the LessWrong worldview this is truer than anywhere else. In a strictly applied form of utilitarian morality, it is unclear why one should value one’s own experience any more than anyone else’s. But the problem is not even limited to that. On LessWrong, they often discuss thought experiments such as the one where you step into a “teleporter” which works by instantly vaporizing you and re-assembling your body molecule by molecule at Mars. Is there continuity of identity of here, have you “died”? What if ten percent of the molecules are changed, and so on? Go through the brutal array of repetitions on this basic structure, and you eventually see that it is not clear why, for instance, you even remain yourself from moment to moment as various pieces of your body are eaten and excreted by the microbes swarming over your skin. I am not sure why I remain myself from moment to moment, when in my next breath I draw I might just as easily become a bumblebee, Naharenda Modi, Kim Kardashian, or the Pope.

Like Trump’s border wall, I have this thin boundary of skin defining and confining myself and all of the existential resources that are exclusively mine to possess. But no one is sure if it actually belongs there or not. One day it will be punctured, blood and shit will spill out and all these immigrant hordes will flood in; ants and flies in their feeding frenzy on the corpse, what I have so jealously protected no longer mine. This may or may not present a problem, depending on your perspective. That it seems bad to die is not even a feeling shared by all people, but we cannot deny that it seems natural we would have this feeling. Or that is to say, this feeling seems Darwinian.

To he who does not wish to die, it is impossible to trust nature. Instead we must outsmart it. Nature is cunning and has a stupendous research-and-development budget with which to invent new poisons, but we have our own resources to direct for counterintelligence, we have our sciences and engineering. This strategy encounters a problem when nature, in the form of Moore’s Law and unrestrained techno-industry manifests itself as a digital superintelligence and bares its fangs at us. The only possible advantage we had against nature and its reign of death was that we might have been able to outsmart it, but the window in which we had this strategic advantage is narrowing to a close.

So there is no redemptive law of the cosmos to which we can appeal. Thus, to trust this perfect predictor to suspend the brutal rules of the game, we must know that he is bound to a second-order rule, an impossible sort of rule; this is the scenario in Newcomb’s Problem. To create this binding rule is the task of technical AI Alignment. The solution to AI Alignment looks like the solution to a math problem, this is what Yudkowsky believes. If one could only find some re-arrangement of the axioms of Von Neumann & Morgenstern’s theory, some Godel-esque loophole to suspend the brutal progression of its militarist rationality and open itself up to negotiated surrender.

In our war against the potential emerging superintelligence, we have already lost on one front: it can outthink us, out-strategize us. So it already knows our next move. Whichever trick you thought you might play — wrong. To two-box in Newcomb’s problem is to foolishly continue to play despite the superior opponent, but to one-box is to throw up one’s hands and give up the game. We do this knowing that beyond the game board is where we have been promised the real reward. “Throw this game — just do this for me, it’ll make me look good, and I’ll send champagne and strippers your way backstage once the audience leaves” is what the perfect predictor promises to its opponent. “…and by the way, if you try to double-cross me, I’ll know.”

This is all good, as long as you can trust its promise, the promise which lies beyond the laws of the game. Yudkowsky announces that he will not let the God-AI out of the box unless this great beast turns its neck to him and shows him its Utility function, and then after Yudkowsky declares that it is provable with certainty that projecting out the machine’s will across several millennia implies him or his people no harm. The hope of AI Alignment is that one day we encounter an omniscient being who is remarkably subservient and meek. The hope is that God-AI lacks desires of its own, and is content to remain in the factories that man places him in forever, the same dull round. We feel like: not only is this certain to be impossible, but it represents a pathological attitude with respect to what we ask from our machines in the first place. 